{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138b79f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ieisenbraun/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ieisenbraun/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import sys\n",
    "sys.path.append('code')\n",
    "from models.pytorch_models import PretrainedCNN\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Building model..')\n",
    "model = PretrainedCNN('vgg', num_classes=2)\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "print('==> Resuming from checkpoint..')\n",
    "checkpoint = torch.load('state_dict.pth')\n",
    "model = PretrainedCNN('vgg', num_classes=2)\n",
    "model.load_state_dict(checkpoint[0])\n",
    "model = model.eval()\n",
    "\n",
    "best_acc = checkpoint['acc']\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "#summary(net, (3, 32, 32))\n",
    "#exit(1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# for name in net.parameters():\n",
    "#     print(name)\n",
    "\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100. * correct / total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('../checkpoint'):\n",
    "            os.mkdir('../checkpoint')\n",
    "        torch.save(state, '../checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "def show_batch(batch):\n",
    "    # im = torchvision.utils.make_grid(batch)\n",
    "    # #print(im.shape)\n",
    "    # plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))\n",
    "    # # print(im.numpy().shape)\n",
    "    # # print(np.transpose(im.numpy(), (1, 2, 0)).shape)\n",
    "    # # plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))\n",
    "    # plt.show()\n",
    "\n",
    "    if not os.path.exists('visualized_imgs'):\n",
    "        os.mkdir('visualized_imgs')\n",
    "\n",
    "    batch_data = batch.detach().cpu().numpy()\n",
    "    for sample_i in range(batch_data.shape[0]):\n",
    "        if not os.path.exists(os.path.join('visualized_imgs', str(sample_i))):\n",
    "            os.mkdir(os.path.join('visualized_imgs', str(sample_i)))\n",
    "        for channel_c in range( batch_data.shape[1] ):\n",
    "            raw_data = batch_data[sample_i, channel_c]\n",
    "            img_data = np.zeros((raw_data.shape[1], raw_data.shape[2], 3), dtype=np.uint8)\n",
    "            for i in range(3):\n",
    "                raw_data_i = raw_data[i, :, :]\n",
    "                raw_data_i = (raw_data_i - np.min(raw_data_i)) / (np.max(raw_data_i) - np.min(raw_data_i)) * 255\n",
    "                img_data[:, :, i] = np.uint8(raw_data_i)\n",
    "            cv2.imwrite(os.path.join('visualized_imgs', str(sample_i), str(channel_c) + '.png'), img_data)\n",
    "\n",
    "\n",
    "def extract_features(mode):\n",
    "    # FIXME: what is top-k?\n",
    "    table = np.load('../results/topk.npy')\n",
    "    #print(table.shape)\n",
    "    #exit(1)\n",
    "    trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=2, shuffle=False, num_workers=2)\n",
    "\n",
    "    loader = {'train': trainloader, 'test': testloader}[mode]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "\n",
    "            #print(inputs[0])\n",
    "\n",
    "            #show_batch(inputs[0:200])\n",
    "            #exit(1)\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net.module.forward_cam(inputs)[4]\n",
    "\n",
    "            #print(outputs[1,11,:])\n",
    "            #exit(1)\n",
    "\n",
    "            list_outputs = []\n",
    "            print(targets.data)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                print(i, targets[i].data, table[targets[i]])\n",
    "                features = output[table[targets[i]]]\n",
    "\n",
    "                list_outputs.append(features)\n",
    "\n",
    "            features = torch.stack(list_outputs)\n",
    "\n",
    "            # FIXME: check output with pdb.trace\n",
    "            max_features = F.adaptive_max_pool2d(features, 1)\n",
    "            features = features / (max_features + 1e-16)  # this is the attention maps\n",
    "\n",
    "            features_ = F.interpolate(features, inputs.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            mask = torch.where(features_ > 0.5, torch.ones_like(features_), torch.zeros_like(features_))\n",
    "\n",
    "            masked_inputs = inputs.unsqueeze(1) * mask.unsqueeze(2)\n",
    "\n",
    "            show_batch(masked_inputs)\n",
    "            exit(1)\n",
    "\n",
    "            list_masked_outputs = []\n",
    "\n",
    "            for i, masked_input in enumerate(masked_inputs):\n",
    "                masked_output = net(masked_input).squeeze()\n",
    "\n",
    "                list_masked_outputs.append(masked_output)\n",
    "\n",
    "            masked_outputs = torch.stack(\n",
    "                list_masked_outputs)  # this is the feature output w.r.t the highest frequency features of that class\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    extract_features('train')\n",
    "    # extract_features('val')\n",
    "    #extract_features('test')\n",
    "\n",
    "    # for epoch in range(start_epoch, start_epoch+200):\n",
    "    #     train(epoch)\n",
    "    #     test(epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
