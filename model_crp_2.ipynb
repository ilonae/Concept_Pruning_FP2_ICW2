{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32eb422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1134707/565847846.py:17: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.utils.prune as prune\n",
    "from operator import itemgetter\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import zennit.image as zimage\n",
    "import torchvision\n",
    "import operator\n",
    "from itertools import islice\n",
    "\n",
    "sys.path.append('code')\n",
    "import imp\n",
    "Lamb = imp.load_source('lamb', '/home/ieisenbraun/Documents/FP/MAI_FINAL_MODEL_VGG/SOURCE/code/utils/lamb.py')\n",
    "from models.pytorch_models import PretrainedCNN\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageOps\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import font_manager\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.attribution import CondAttribution,AttributionGraph\n",
    "from crp.helper import get_layer_names,abs_norm\n",
    "from crp.graph import trace_model_graph\n",
    "\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print(device)\n",
    "#print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d541c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ieisenbraun/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ieisenbraun/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PretrainedCNN(\n",
      "  (model_ft): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): ReLU(inplace=True)\n",
      "      (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (24): ReLU(inplace=True)\n",
      "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('state_dict.pth')['models_state_dict']\n",
    "model = PretrainedCNN('vgg', num_classes=2)\n",
    "model.load_state_dict(state_dict[0])\n",
    "model = model.eval()\n",
    "#model.eval()\n",
    "#eval_loss, eval_accuracy = evaluate_model(model=model,test_loader=test_loader, device=device, criterion=criterion)\n",
    "print(model)\n",
    "#summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5474595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_ft.features.0', 'model_ft.features.4', 'model_ft.features.8', 'model_ft.features.11', 'model_ft.features.15', 'model_ft.features.18', 'model_ft.features.22', 'model_ft.features.25', 'model_ft.classifier.0', 'model_ft.classifier.3', 'model_ft.classifier.6']\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "cc = ChannelConcept()\n",
    "composite = EpsilonPlusFlat([SequentialMergeBatchNorm()])\n",
    "attribution = CondAttribution(model)\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "layer_names = get_layer_names(model, [torch.nn.Conv2d, torch.nn.Linear])\n",
    "# find a font file\n",
    "print(layer_names)\n",
    "font = font_manager.FontProperties(family='sans-serif', weight='bold')\n",
    "fontfile = font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af8c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_images(path,iteration,is_pruned,folder,descending=True):\n",
    "    condition = [{'y': [1]}]\n",
    "    concept_atlas={}\n",
    "  \n",
    "    for file in os.listdir(path):\n",
    "        imgs={}\n",
    "        path_to_file = os.path.join(path,file)\n",
    "        splitpath = path_to_file.rsplit('/', 1)[-1]\n",
    "        splitname = splitpath.rsplit(\".\",1)[0]\n",
    "        image = Image.open(path_to_file).convert('RGB')\n",
    "        sample = transform(image).unsqueeze(0)\n",
    "        sample.requires_grad = True\n",
    "        sample = sample.to(device)\n",
    "        attr = attribution(sample, condition, composite, record_layer=layer_names)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        for model_layer in layer_names:\n",
    "            name,section, _ = model_layer.split(\".\")\n",
    "            single_layer = model_layer[model_layer.index('.') + 1 : ]\n",
    "            for module_name, module in model._modules[name].named_modules():\n",
    "                if single_layer == module_name:\n",
    "                    rel_c = cc.attribute(attr.relevances[model_layer], abs_norm=True)\n",
    "                    concept_ids = torch.argsort(rel_c[0], descending=descending)[:10]\n",
    "                    \n",
    "                    conditions = [{model_layer: [id], 'y': [1]} for id in concept_ids]\n",
    "                    heatmap, _, _, _ = attribution(sample, conditions, composite)\n",
    "                    img=(zimage.imgify(heatmap, symmetric=True, grid=(1, len(concept_ids)))) \n",
    "                    imgs[module_name]=img\n",
    "                    \n",
    "                    del conditions\n",
    "                    \n",
    "                    for concept in concept_ids:\n",
    "                        concept = concept.cpu().item()\n",
    "                        if ( (single_layer not in concept_atlas)\n",
    "                           or\n",
    "                           (concept not in concept_atlas[single_layer])) :\n",
    "                            concept_atlas.setdefault(single_layer, dict())[concept] =1\n",
    "                        else:\n",
    "                            concept_atlas[single_layer][concept]+=1 \n",
    "                            \n",
    "                    del concept_ids\n",
    "                    torch.cuda.empty_cache() \n",
    "                    \n",
    "        del attr\n",
    "        del sample\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "        vis_imgs(iteration,is_pruned,imgs,image,splitname,folder)\n",
    "    return concept_atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6518fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def nested_dict_iter(nested):\n",
    "    for key, value in nested.items():\n",
    "        if isinstance(value, collections.Mapping):\n",
    "            for inner_key, inner_value in nested_dict_iter(value):\n",
    "                yield key, inner_key, inner_value\n",
    "        else:\n",
    "            yield key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f64b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7ff656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sample_attributions(iteration,is_pruned):\n",
    "    prunable_channels={}\n",
    "    true_images_path='examples_ds_from_MP.train.HTW.train/True'\n",
    "    false_images_path='examples_ds_from_MP.train.HTW.train/False'\n",
    "    true_concept_atlas = iterate_images(true_images_path, iteration,is_pruned, 'true_attributions', False)\n",
    "    false_concept_atlas=iterate_images(false_images_path,iteration,is_pruned,'false_attributions',True)\n",
    "    #print(false_concept_atlas)\n",
    "    torch.cuda.empty_cache()\n",
    "    irrelevant_channels_dict={}\n",
    "    most_irrelevant_channels_dict={}\n",
    "    prunable_channels ={}\n",
    "    \n",
    "    for layer in false_concept_atlas:\n",
    "        #print(false_concept_atlas[layer])\n",
    "        final_dict = dict(true_concept_atlas[layer].items() & false_concept_atlas[layer].items())\n",
    "        #print (\"final dictionary\", str(final_dict))\n",
    "        irrelevant_channels_dict[layer] = {k: false_concept_atlas[layer][k] for k in false_concept_atlas[layer]\n",
    "                                      if k not in final_dict}   \n",
    "        #print(irrelevant_channels_dict[layer])\n",
    "        irrelevant_channels_dict[layer] = dict( sorted(irrelevant_channels_dict[layer].items(),\n",
    "                                                       key=operator.itemgetter(1),reverse=True))\n",
    "        #print(irrelevant_channels_dict[layer])\n",
    "      \n",
    "    del true_concept_atlas\n",
    "    del false_concept_atlas\n",
    "    torch.cuda.empty_cache()\n",
    "    irrelevant_channels_dict_lst = list(nested_dict_iter((irrelevant_channels_dict)))\n",
    "    sorting_record = sorted(irrelevant_channels_dict_lst, key = lambda i: i[2], reverse = True)[0:10]\n",
    "    \n",
    "    for item in sorting_record:\n",
    "        if (item[0] not in prunable_channels):\n",
    "            prunable_channels[item[0]] =[item[1]]\n",
    "        else:\n",
    "            prunable_channels[item[0]].append(item[1])\n",
    "         \n",
    "    #for layer in irrelevant_channels_dict:\n",
    "        \n",
    "        #most_irrelevant_channels_dict[layer] = take(2, irrelevant_channels_dict[layer].items())\n",
    "        #print(most_irrelevant_channels_dict[layer])\n",
    "        #prunable_channels[layer] = list(most_irrelevant_channels_dict[layer].keys())\n",
    "\n",
    "    return prunable_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad694c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(eval_model):\n",
    "    # All Mitoses\n",
    "    path = 'examples_ds_from_MP.train.HTW.train/True'\n",
    "    model_pred =0\n",
    "    for file in os.listdir(path):\n",
    "        path_to_file = os.path.join(path,file)  \n",
    "        img = cv2.imread(path_to_file)\n",
    "        # Preprocess for the model\n",
    "        original = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img2 = original[144:144+224,144:144+224]\n",
    "        img = img2 / 255.\n",
    "        img -= [0.86121925, 0.86814779, 0.88314296] # MEAN-Values\n",
    "        img /= [0.13475281, 0.10909398, 0.09926313] # STD-Values\n",
    "        imgtensor = torch.tensor(img.swapaxes(1,2).swapaxes(0,1),dtype=torch.float32)[None]\n",
    "        pred = eval_model.predict(imgtensor.to(device),logits=True).detach().cpu()\n",
    "        #print(pred)\n",
    "        pred = float(torch.nn.functional.softmax(pred)[0,1].numpy())\n",
    "        #print(pred)\n",
    "        model_pred+=pred\n",
    "\n",
    "        \n",
    "    model_pred = model_pred / len(os.listdir(path))\n",
    "    return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daecdaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return dict(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fb443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_non_empty(my_dict):\n",
    "    temp_dict = {}\n",
    "    for k, v in my_dict.items():\n",
    "        if v:\n",
    "            if isinstance(v, dict):\n",
    "                return_dict = return_non_empty(v)\n",
    "                if return_dict:\n",
    "                    temp_dict[k] = return_dict\n",
    "                else:\n",
    "                    temp_dict[k] = v\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca952e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_ids(test_dict: dict):\n",
    "    max_vote_ids={}\n",
    "    while sum(len(lst) for lst in max_vote_ids.values())<10:\n",
    "        \n",
    "        for layer in test_dict:\n",
    "            max_votes =  take(2, test_dict[layer].items())\n",
    "            max_vote_ids[layer].append(k_inner)\n",
    "            print(max_votes, layer)\n",
    "                \n",
    "    return max_vote_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e1c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_selected_concepts(max_vote_ids,model):\n",
    "    for k,v in max_vote_ids.items():\n",
    "        section, no = k.split(\".\")\n",
    "        #v = v.cpu().data.numpy()\n",
    "        name = \"model_ft\"\n",
    "        #print(name,section,no)\n",
    "        for module_name, module in model._modules[name].named_modules():\n",
    "            #print(module_name)\n",
    "            #print(k)\n",
    "            nr = int(k.split('.', 1)[1])\n",
    "            mod = k.split('.', 1)[0]\n",
    "            if mod == module_name:\n",
    "                #print(k)\n",
    "                for concept in v:\n",
    "                    #print(concept)\n",
    "\n",
    "                    mask_tensor = torch.ones(module[nr].weight.shape, device='cuda:0')\n",
    "                    #print(module[nr])\n",
    "                    #print(module[nr].weight.shape)\n",
    "                    \n",
    "                    #pruning differences between Conv2d and Linear layer\n",
    "                    if isinstance(module[nr], nn.Conv2d):\n",
    "                        #masking channel by id so nothing is passed further\n",
    "                        mask_tensor[concept, :, :, :] = torch.zeros(module[nr].weight[0].shape)\n",
    "                    elif isinstance(module[nr], nn.Linear):\n",
    "                        mask_tensor[concept,:] = torch.zeros(module[nr].weight[0].shape)\n",
    "                        #mask_tensor=mask_tensor\n",
    "\n",
    "                    m = prune.custom_from_mask(module[nr], name='weight', mask=mask_tensor)\n",
    "                    #print(m.weight_mask)\n",
    "                    prune.remove(module[nr],\"weight\")\n",
    "            torch.cuda.empty_cache()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cecc769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irrelevant_concepts(prunable_channels,iteration):\n",
    "    path = 'examples_ds_from_MP.train.HTW.train/True'\n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "  \n",
    "        imgs={}\n",
    "        \n",
    "        path_to_file = os.path.join(path,file)\n",
    "        splitpath = path_to_file.rsplit('/', 1)[-1]\n",
    "        splitname = splitpath.rsplit(\".\",1)[0]\n",
    "        image = Image.open(path_to_file).convert('RGB')\n",
    "        sample = transform(image).unsqueeze(0)\n",
    "        sample.requires_grad = True\n",
    "        sample = sample.to(device)\n",
    "        \n",
    "        imgs = {}\n",
    "\n",
    "        for layer in prunable_channels:\n",
    "            conditions = [{'model_ft.'+layer: [torch.tensor(id).to(device)], 'y': [0]} for id in prunable_channels[layer]]\n",
    "            if len(conditions):\n",
    "                heatmap, _, _, _ = attribution(sample, conditions, composite)\n",
    "                img=(zimage.imgify(heatmap, symmetric=True, grid=(1, len(prunable_channels[layer])))) \n",
    "                imgs[layer]=img\n",
    "            del conditions\n",
    "            del attribution\n",
    "        \n",
    "        del sample\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "        vis_imgs(iteration,False,imgs,image,splitname,'irrelevant_concepts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f33c3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_imgs(iteration,pruned_bool,true_imgs,sample,samplename,path):\n",
    "    fontsize=1\n",
    "    img_fraction=0.1\n",
    "    font = ImageFont.truetype(fontfile, fontsize)\n",
    "    \n",
    "    if pruned_bool:\n",
    "        flag = '_after_pruning_'\n",
    "    else:\n",
    "        flag = '_before_pruning_'\n",
    "        \n",
    "    min_img_width = min(i.width for i in true_imgs.values())\n",
    "    total_height = 0\n",
    "    img=0\n",
    "    for i, key in enumerate(true_imgs.copy()):\n",
    "        img = true_imgs[key]\n",
    "        img.convert('RGB')\n",
    "        img = ImageOps.expand(img, border=int(0.2*img.size[1]), fill=(255,255,255))\n",
    "        while font.getsize(key)[0] < img_fraction*img.size[0]:\n",
    "            # iterate until the text size is just larger than the criteria\n",
    "            fontsize += 1\n",
    "            font = ImageFont.truetype(fontfile, fontsize)\n",
    "        fontsize -= 1\n",
    "        font = ImageFont.truetype(fontfile, fontsize)\n",
    "        d = ImageDraw.Draw(img)\n",
    "        w,h = font.getsize(key)\n",
    "        d.text((int((img.width-w)/2),0), key, fill=(0,0,0),font=font)\n",
    "        if img.width > min_img_width:\n",
    "            true_imgs[key] = img.resize((min_img_width, int(img.height / img.width * min_img_width)), Image.ANTIALIAS)\n",
    "        total_height += true_imgs[key].height\n",
    "        \n",
    "\n",
    "    wpercent = (min_img_width/float(sample.size[0]))\n",
    "    hsize = int((float(sample.size[1])*float(wpercent)))\n",
    "    sample = sample.resize((min_img_width,hsize), Image.ANTIALIAS)\n",
    "    \n",
    "    sample = ImageOps.expand(sample,  border=int(0.1*sample.size[1]), fill=(255,255,255))\n",
    "    d = ImageDraw.Draw(sample)\n",
    "    w,h = font.getsize(\"Original\")\n",
    "    d.text((int((sample.width-w)/2),0), \"Original\", fill=(0,0,0),font=font)\n",
    "    \n",
    "    img_merge = Image.new(img.mode, (min_img_width+sample.width+(2*int(0.1*sample.size[1])), total_height)).convert('RGB')\n",
    "    img_merge.paste((255,255,255), [0,0,min_img_width+sample.width+2*int(0.1*sample.size[1]),total_height])\n",
    "    y = 0\n",
    "    for image in true_imgs.values():\n",
    "        img_merge.paste(image, (0, y))\n",
    "\n",
    "        y += image.height\n",
    "    img_merge.paste(sample, (sample.width, int(sample.height/2)-2*int(0.1*sample.size[1])))\n",
    "    img_merge.save(path+'/iteration_'+str(iteration)+str(flag)+samplename+'.jpg')\n",
    "    ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742f9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(train_loader,test_loader,model,optimizer, criterion,epochs):\n",
    "    \n",
    "    train_len = len(train_loader)    \n",
    "    test_len = len(test_loader)\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        total_val_loss = 0.0\n",
    "        model_pred =0\n",
    "        # Training the model\n",
    "        model.train()\n",
    "        counter = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = model(inputs)\n",
    "            log_prob = torch.nn.functional.log_softmax(output, dim=1)\n",
    "            loss = torch.nn.functional.nll_loss(log_prob, labels)\n",
    "            #optimizer.zero_grad()\n",
    "            #outputs = model.forward(inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Evaluating the model\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  \n",
    "                \n",
    "                \n",
    "                output = model(inputs)\n",
    "                val_log_prob = torch.nn.functional.log_softmax(output, dim=1)\n",
    "                val_loss = torch.nn.functional.nll_loss(val_log_prob, labels)\n",
    "            \n",
    "                total_val_loss += val_loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = train_loss/train_len\n",
    "        valid_loss = total_val_loss/test_len\n",
    "        #print('[%d] Training Loss: %.6f, Validation Loss: %.6f'  % (epoch + 1, train_loss, valid_loss))\n",
    "    torch.cuda.empty_cache()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84df6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = torch.utils.data.Subset(dataset, train_idx)\n",
    "    datasets['val'] = torch.utils.data.Subset(dataset, val_idx)\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4842a383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'False': 0, 'True': 1}\n"
     ]
    }
   ],
   "source": [
    "path = 'examples_ds_from_MP.train.HTW.train'\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0),\n",
    "                                       #transforms.Resize(317),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomRotation(30),\n",
    "                                       #transforms.CenterCrop(224),\n",
    "                                       transforms.Normalize(\n",
    "                                           mean=[0.86121925, 0.86814779, 0.88314296], \n",
    "                                           std=[0.13475281, 0.10909398, 0.09926313])\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                                     #transforms.Resize(317),\n",
    "                                      # transforms.CenterCrop(224),\n",
    "                                      transforms.Normalize(\n",
    "                                           mean=[0.86121925, 0.86814779, 0.88314296], \n",
    "                                           std=[0.13475281, 0.10909398, 0.09926313]),\n",
    "                                     transforms.ToTensor()])\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(path, transform=train_transforms)\n",
    "test_data = torchvision.datasets.ImageFolder(path, transform=test_transforms)\n",
    "\n",
    "\n",
    "#print(len(train_data))\n",
    "datasets = train_val_dataset(train_data)\n",
    "print(train_data.class_to_idx)\n",
    "#print(datasets['val'][5])\n",
    "# The original dataset is available in the Subset class\n",
    "#print(datasets['train'].dataset)\n",
    "\n",
    "dataloaders = {x:torch.utils.data.DataLoader(datasets[x],32, shuffle=True, num_workers=4) for x in ['train','val']}\n",
    "#x,y = next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db5973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1134707/4226076107.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = float(torch.nn.functional.softmax(pred)[0,1].numpy())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial pred: 0.8473105363547802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ieisenbraun/anaconda3/lib/python3.8/site-packages/zennit/image.py:184: RuntimeWarning: invalid value encountered in true_divide\n",
      "  array = (array - vmin) / (vmax - vmin)\n",
      "/tmp/ipykernel_1134707/68766418.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  if isinstance(value, collections.Mapping):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voted least relevant concept ids to prune (max vote ids): {'features.0': [30, 29, 24, 26, 19, 18, 20, 23, 35, 33]}\n",
      "iteration 0 , pred after pruning:  0.8418153431266546\n",
      "iteration 0 , pred after pruning AND finetuning:  0.9132532857358455\n",
      "Voted least relevant concept ids to prune (max vote ids): {'features.15': [222], 'classifier.3': [983], 'classifier.0': [2061, 3898, 3571, 1524, 1153], 'features.4': [55], 'features.0': [22, 53]}\n",
      "iteration 1 , pred after pruning:  0.9099357143044472\n",
      "iteration 1 , pred after pruning AND finetuning:  0.9030077219009399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = dataloaders['train']\n",
    "test_loader = dataloaders['val']\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_loader, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_loader, batch_size=32,shuffle=True)\n",
    "optimizer = Lamb.Lamb(model.parameters(), lr=0.00025, weight_decay=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "\n",
    "f = open('./result_log.txt', 'w') \n",
    "f.write(\"Concept Pruning log\")\n",
    "f.write(\"\\n=========================================================================================\")\n",
    "pred= evaluate_model(model)\n",
    "print(\"\\nInitial pred: {}\".format(pred))\n",
    "f.write(\"\\nInitial pred: {}\".format(pred))\n",
    "for i in range(0,5):\n",
    "    f.write(\"\\n=========================================================================================\")\n",
    "    f.write(\"\\niteration {} , finding irrelevant concepts...\".format(i))\n",
    "    prunable_channels=find_sample_attributions(i,False)\n",
    "    print(\"Voted least relevant concept ids to prune (max vote ids): {}\".format(prunable_channels))\n",
    "    f.write(\"\\nVoted least relevant concept ids to prune (max vote ids): {}\".format(prunable_channels))\n",
    "   \n",
    "    #print(\"Visualising irrelevant concepts...\")\n",
    "    #f.write(\"Visualising irrelevant concepts...\")\n",
    "    #irrelevant_concepts(prunable_channels,i)\n",
    "    \n",
    "    model = prune_selected_concepts(prunable_channels,model)\n",
    "    pred = evaluate_model(model)\n",
    "    print(\"iteration {} , pred after pruning:  {}\".format(i, pred))\n",
    "    f.write(\"\\niteration {} , pred after pruning:  {}\".format(i, pred))\n",
    "    \n",
    "    \n",
    "    model = fine_tune_model(train_loader,test_loader,model,optimizer, criterion,epochs)\n",
    " \n",
    "    pred = evaluate_model(model)\n",
    "    print(\"iteration {} , pred after pruning AND finetuning:  {}\".format(i, pred))\n",
    "    f.write(\"\\niteration {} , pred after pruning AND finetuning:  {}\".format(i, pred))\n",
    "    \n",
    "    #prunable_channels=find_sample_attributions(i,True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    f.write(\"\\n=========================================================================================\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caabb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6bd1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
