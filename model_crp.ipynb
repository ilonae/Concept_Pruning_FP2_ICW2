{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32eb422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import zennit.image as zimage\n",
    "import torchvision\n",
    "\n",
    "sys.path.append('code')\n",
    "from models.pytorch_models import PretrainedCNN\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageOps\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import font_manager\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.attribution import CondAttribution,AttributionGraph\n",
    "from crp.helper import get_layer_names,abs_norm\n",
    "from crp.graph import trace_model_graph\n",
    "\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print(device)\n",
    "#print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d541c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ieisenbraun/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ieisenbraun/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('state_dict.pth')['models_state_dict']\n",
    "model = PretrainedCNN('vgg', num_classes=2)\n",
    "model.load_state_dict(state_dict[0])\n",
    "model = model.eval()\n",
    "#model.eval()\n",
    "#eval_loss, eval_accuracy = evaluate_model(model=model,test_loader=test_loader, device=device, criterion=criterion)\n",
    "#print(model)\n",
    "#summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5474595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "cc = ChannelConcept()\n",
    "composite = EpsilonPlusFlat([SequentialMergeBatchNorm()])\n",
    "attribution = CondAttribution(model)\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "# find a font file\n",
    "font = font_manager.FontProperties(family='sans-serif', weight='bold')\n",
    "fontfile = font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7ff656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sample_attributions(path_to_file, prunable_channels,iteration,pruned_bool):\n",
    "    splitpath = path_to_file.rsplit('/', 1)[-1]\n",
    "    splitname = splitpath.rsplit(\".\",1)[0]\n",
    "    #Attribute Concepts\n",
    "    #Recording and Attributing Latent Concept Relevances\n",
    "    true_imgs={}\n",
    "    false_imgs={}\n",
    "    image = Image.open(path_to_file).convert('RGB')\n",
    "    sample = transform(image).unsqueeze(0)\n",
    "    sample.requires_grad = True\n",
    "    sample = sample.to(device)\n",
    "    layer_names = get_layer_names(model, [torch.nn.Conv2d, torch.nn.Linear])\n",
    "    true_conditions = [{'y': [1]}]\n",
    "    false_conditions = [{'y': [0]}]\n",
    "    true_attr = attribution(sample, true_conditions, composite, record_layer=layer_names)\n",
    "    false_attr = attribution(sample, false_conditions, composite, record_layer=layer_names)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for model_layer in layer_names:\n",
    "        name,section, _ = model_layer.split(\".\")\n",
    "        single_layer = model_layer[model_layer.index('.') + 1 : ]\n",
    "        for module_name, module in model._modules[name].named_modules():\n",
    "            if single_layer == module_name:\n",
    "\n",
    "                true_rel_c = cc.attribute(true_attr.relevances[model_layer], abs_norm=True)\n",
    "                false_rel_c = cc.attribute(false_attr.relevances[model_layer], abs_norm=True)\n",
    "\n",
    "                # the ten most irrelevant concepts and their percentage for y = 1 (true)\n",
    "                true_concept_ids = torch.argsort(true_rel_c[0], descending=True)[:10]\n",
    "                true_contributions =  abs_norm(true_rel_c[0])[true_concept_ids]*100\n",
    "                # conditioned heatmap for specific layer and concept id in loop (for y=1)\n",
    "                conditions = [{model_layer: [id], 'y': [1]} for id in true_concept_ids]\n",
    "                heatmap, _, _, _ = attribution(sample, conditions, composite)\n",
    "                true_img=(zimage.imgify(heatmap, symmetric=True, grid=(1, len(true_concept_ids)))) \n",
    "                true_imgs[module_name]=true_img\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # the ten most relevant concepts and their percentage for y = 0 (false)\n",
    "                false_concept_ids = torch.argsort(false_rel_c[0], descending=True)[:10]\n",
    "                false_contributions =  abs_norm(false_rel_c[0])[false_concept_ids]*100\n",
    "                conditions = [{model_layer: [id], 'y': [0]} for id in true_concept_ids]\n",
    "                heatmap, _, _, _ = attribution(sample, conditions, composite)\n",
    "                false_img=(zimage.imgify(heatmap, symmetric=True, grid=(1, len(false_concept_ids)))) \n",
    "                false_imgs[module_name]=false_img\n",
    "                torch.cuda.empty_cache()\n",
    "                #print(true_concept_ids)\n",
    "                #print(false_concept_ids)\n",
    "\n",
    "                #check which of the irrelevant true concepts are NOT the relevant false concepts\n",
    "                #diff_concept_ids = list(set([int(t.item()) for t in true_concept_ids]) - set([int(t.item()) for t in false_concept_ids]))\n",
    "                 \n",
    "                for concept in true_concept_ids:\n",
    "                    if ( (model_layer not in prunable_channels)\n",
    "                       or\n",
    "                       (concept not in prunable_channels[model_layer])) :\n",
    "                        prunable_channels.setdefault(model_layer, dict())[concept] =1\n",
    "                    else:\n",
    "                        prunable_channels[model_layer][concept]+=1 \n",
    "    vis_imgs(iteration,pruned_bool,true_imgs,false_imgs,image,splitname)\n",
    "    return prunable_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad694c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(eval_model,iteration,pruned_bool):\n",
    "    # All Mitoses\n",
    "    path = 'examples_ds_from_MP.train.HTW.train/True'\n",
    "    model_pred =0\n",
    "    prunable_channels={}\n",
    "    for file in os.listdir(path):\n",
    "        path_to_file = os.path.join(path,file)\n",
    "        prunable_channels = find_sample_attributions(path_to_file,prunable_channels,iteration,pruned_bool)\n",
    "        img = cv2.imread(path_to_file)\n",
    "        # Preprocess for the model\n",
    "        original = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img2 = original[144:144+224,144:144+224]\n",
    "        img = img2 / 255.\n",
    "        img -= [0.86121925, 0.86814779, 0.88314296] # MEAN-Values\n",
    "        img /= [0.13475281, 0.10909398, 0.09926313] # STD-Values\n",
    "        imgtensor = torch.tensor(img.swapaxes(1,2).swapaxes(0,1),dtype=torch.float32)[None]\n",
    "        pred = eval_model.predict(imgtensor.to(device),logits=True).detach().cpu()\n",
    "        pred = float(torch.nn.functional.softmax(pred)[0,1].numpy())\n",
    "        #add overall prediction confidence\n",
    "        model_pred+=pred\n",
    "    model_pred = model_pred / len(os.listdir(path))\n",
    "    return prunable_channels, model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22fb443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_non_empty(my_dict):\n",
    "    temp_dict = {}\n",
    "    for k, v in my_dict.items():\n",
    "        if v:\n",
    "            if isinstance(v, dict):\n",
    "                return_dict = return_non_empty(v)\n",
    "                if return_dict:\n",
    "                    temp_dict[k] = return_dict\n",
    "                else:\n",
    "                    temp_dict[k] = v\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca952e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_ids(test_dict: dict):\n",
    "    max_vote_ids={}\n",
    "    while sum(len(lst) for lst in max_vote_ids.values())<5:\n",
    "        # step 1\n",
    "        k_outer, v_outer = max(test_dict.items(), key=lambda x: max(x[1]),default=0)\n",
    "        # step 2\n",
    "        k_inner, v_inner = max(v_outer.items(), key=lambda x: x[1],default=0)\n",
    "        if k_outer not in max_vote_ids:\n",
    "            max_vote_ids.update( {k_outer : [k_inner]} )\n",
    "        else:\n",
    "            max_vote_ids[k_outer].append(k_inner)\n",
    "        test_dict[k_outer].pop(k_inner, None)\n",
    "        test_dict=return_non_empty(test_dict)      \n",
    "    return max_vote_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e1c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_selected_concepts(max_vote_ids,model):\n",
    "    for k,v in max_vote_ids.items():\n",
    "        name,section, no = k.split(\".\")\n",
    "        #print(name,section,no)\n",
    "        for module_name, module in model._modules[name].named_modules():\n",
    "            #print(module_name)\n",
    "            #print(k)\n",
    "            if k.split('.', 1)[1] == module_name:\n",
    "                #print(k)\n",
    "                for concept in v:\n",
    "\n",
    "                    mask_tensor = torch.ones(module.weight.shape, device='cuda:0')\n",
    "                    #pruning differences between Conv2d and Linear layer\n",
    "                    if isinstance(module, nn.Conv2d):\n",
    "                        #masking channel by id so nothing is passed further\n",
    "                        mask_tensor[concept, :, :, :] = torch.zeros(module.weight[0].shape)\n",
    "                    elif isinstance(module, nn.Linear):\n",
    "                        mask_tensor[concept,:] = torch.zeros(module.weight[0].shape)\n",
    "                        #mask_tensor=mask_tensor\n",
    "\n",
    "                    m = prune.custom_from_mask(module, name='weight', mask=mask_tensor)\n",
    "                    #print(m.weight_mask)\n",
    "                    prune.remove(module,\"weight\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f33c3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_imgs(iteration,pruned_bool,true_imgs, false_imgs,sample,samplename):\n",
    "    fontsize=1\n",
    "    img_fraction=0.1\n",
    "    font = ImageFont.truetype(fontfile, fontsize)\n",
    "    \n",
    "    if pruned_bool:\n",
    "        flag = '_after_pruning_'\n",
    "    else:\n",
    "        flag = '_before_pruning_'\n",
    "        \n",
    "    min_img_width = min(i.width for i in true_imgs.values())\n",
    "    total_height = 0\n",
    "    img=0\n",
    "    for i, key in enumerate(true_imgs.copy()):\n",
    "        img = true_imgs[key]\n",
    "        img.convert('RGB')\n",
    "        img = ImageOps.expand(img, border=int(0.2*img.size[1]), fill=(255,255,255))\n",
    "        while font.getsize(key)[0] < img_fraction*img.size[0]:\n",
    "            # iterate until the text size is just larger than the criteria\n",
    "            fontsize += 1\n",
    "            font = ImageFont.truetype(fontfile, fontsize)\n",
    "        fontsize -= 1\n",
    "        font = ImageFont.truetype(fontfile, fontsize)\n",
    "        d = ImageDraw.Draw(img)\n",
    "        w,h = font.getsize(key)\n",
    "        d.text((int((img.width-w)/2),0), key, fill=(0,0,0),font=font)\n",
    "        if img.width > min_img_width:\n",
    "            true_imgs[key] = img.resize((min_img_width, int(img.height / img.width * min_img_width)), Image.ANTIALIAS)\n",
    "        total_height += true_imgs[key].height\n",
    "        \n",
    "\n",
    "    wpercent = (min_img_width/float(sample.size[0]))\n",
    "    hsize = int((float(sample.size[1])*float(wpercent)))\n",
    "    sample = sample.resize((min_img_width,hsize), Image.ANTIALIAS)\n",
    "    \n",
    "    sample = ImageOps.expand(sample,  border=int(0.1*sample.size[1]), fill=(255,255,255))\n",
    "    d = ImageDraw.Draw(sample)\n",
    "    w,h = font.getsize(\"Original\")\n",
    "    d.text((int((sample.width-w)/2),0), \"Original\", fill=(0,0,0),font=font)\n",
    "    \n",
    "    img_merge = Image.new(img.mode, (min_img_width+sample.width+(2*int(0.1*sample.size[1])), total_height)).convert('RGB')\n",
    "    img_merge.paste((255,255,255), [0,0,min_img_width+sample.width+2*int(0.1*sample.size[1]),total_height])\n",
    "    y = 0\n",
    "    for image in true_imgs.values():\n",
    "        img_merge.paste(image, (0, y))\n",
    "\n",
    "        y += image.height\n",
    "    img_merge.paste(sample, (sample.width, int(sample.height/2)-2*int(0.1*sample.size[1])))\n",
    "    img_merge.save('true_attributions/true_iteration_'+str(iteration)+str(flag)+samplename+'.jpg')\n",
    "    ##########################\n",
    "\n",
    "    min_img_width = min(i.width for i in false_imgs.values())\n",
    "    total_height = 0\n",
    "    img=0\n",
    "    for i, key in enumerate(false_imgs.copy()):\n",
    "        img = false_imgs[key]\n",
    "        img.convert('RGB')\n",
    "        img = ImageOps.expand(img, border=int(0.2*img.size[1]), fill=(255,255,255))\n",
    "        while font.getsize(key)[0] < img_fraction*img.size[0]:\n",
    "            # iterate until the text size is just larger than the criteria\n",
    "            fontsize += 1\n",
    "            font = ImageFont.truetype(fontfile, fontsize)\n",
    "        fontsize -= 1\n",
    "        font = ImageFont.truetype(fontfile, fontsize)\n",
    "        d = ImageDraw.Draw(img)\n",
    "        w,h = font.getsize(key)\n",
    "        d.text((int((img.width-w)/2),0), key, fill=(0,0,0),font=font)\n",
    "        if img.width > min_img_width:\n",
    "            false_imgs[key] = img.resize((min_img_width, int(img.height / img.width * min_img_width)), Image.ANTIALIAS)\n",
    "        total_height += false_imgs[key].height\n",
    "    \n",
    "    img_merge = Image.new(img.mode, (min_img_width+sample.width+(2*int(0.1*sample.size[1])), total_height)).convert('RGB')\n",
    "    img_merge.paste((255,255,255), [0,0,min_img_width+sample.width+2*int(0.1*sample.size[1]),total_height])\n",
    "    y = 0\n",
    "    for image in false_imgs.values():\n",
    "        img_merge.paste(image, (0, y))\n",
    "\n",
    "        y += image.height\n",
    "    img_merge.paste(sample, (sample.width, int(sample.height/2)-2*int(0.1*sample.size[1])))\n",
    "    img_merge.save('false_attributions/false_iteration_'+str(iteration)+str(flag)+samplename+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "742f9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(train_loader,test_loader,model,optimizer, criterion,epochs):\n",
    "    model.train()\n",
    "    train_len = len(train_loader)    \n",
    "    test_len = len(test_loader)\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # Training the model\n",
    "        model.train()\n",
    "        counter = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Evaluating the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)           \n",
    "                output = model.forward(inputs)            \n",
    "                valloss = criterion(output, labels)\n",
    "                val_loss += valloss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = train_loss/train_len\n",
    "        valid_loss = val_loss/test_len\n",
    "        print('[%d] Training Loss: %.6f, Validation Loss: %.6f'  % (epoch + 1, train_loss, valid_loss))\n",
    "    torch.cuda.empty_cache()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84df6101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "60\n",
      "(tensor([[[0.9961, 0.9961, 0.9961,  ..., 0.9294, 0.9255, 0.9294],\n",
      "         [0.9961, 0.9961, 0.9961,  ..., 0.9255, 0.9255, 0.9294],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.9216, 0.9176, 0.9255],\n",
      "         ...,\n",
      "         [0.9647, 0.9569, 0.9686,  ..., 0.9686, 0.9451, 0.9373],\n",
      "         [0.9843, 0.9647, 0.9686,  ..., 0.9765, 0.9608, 0.9569],\n",
      "         [0.9961, 0.9804, 0.9725,  ..., 0.9804, 0.9765, 0.9765]],\n",
      "\n",
      "        [[0.9922, 0.9922, 0.9922,  ..., 0.9216, 0.9216, 0.9255],\n",
      "         [0.9922, 0.9922, 0.9922,  ..., 0.9176, 0.9216, 0.9255],\n",
      "         [0.9961, 0.9961, 0.9961,  ..., 0.9098, 0.9137, 0.9216],\n",
      "         ...,\n",
      "         [0.9490, 0.9333, 0.9373,  ..., 0.9373, 0.9255, 0.9294],\n",
      "         [0.9804, 0.9529, 0.9490,  ..., 0.9529, 0.9451, 0.9529],\n",
      "         [1.0000, 0.9725, 0.9647,  ..., 0.9647, 0.9647, 0.9725]],\n",
      "\n",
      "        [[0.9765, 0.9765, 0.9765,  ..., 0.9059, 0.9137, 0.9176],\n",
      "         [0.9765, 0.9765, 0.9765,  ..., 0.9020, 0.9137, 0.9176],\n",
      "         [0.9804, 0.9804, 0.9804,  ..., 0.8941, 0.9059, 0.9137],\n",
      "         ...,\n",
      "         [0.9647, 0.9490, 0.9569,  ..., 0.9333, 0.9255, 0.9294],\n",
      "         [0.9882, 0.9647, 0.9647,  ..., 0.9451, 0.9412, 0.9412],\n",
      "         [1.0000, 0.9843, 0.9765,  ..., 0.9490, 0.9490, 0.9569]]]), 1)\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 80\n",
      "    Root location: examples_ds_from_MP.train.HTW.train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomRotation(degrees=[-30.0, 30.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = torch.utils.data.Subset(dataset, train_idx)\n",
    "    datasets['val'] = torch.utils.data.Subset(dataset, val_idx)\n",
    "    return datasets\n",
    "\n",
    "path = 'examples_ds_from_MP.train.HTW.train'\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                      transforms.RandomResizedCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                     transforms.RandomResizedCrop(224),\n",
    "                                     transforms.ToTensor()])\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(path, transform=train_transforms)\n",
    "test_data = torchvision.datasets.ImageFolder(path, transform=test_transforms)\n",
    "\n",
    "\n",
    "print(len(train_data))\n",
    "datasets = train_val_dataset(train_data)\n",
    "print(len(datasets['train']))\n",
    "print(datasets['val'][5])\n",
    "# The original dataset is available in the Subset class\n",
    "print(datasets['train'].dataset)\n",
    "\n",
    "dataloaders = {x:torch.utils.data.DataLoader(datasets[x],32, shuffle=True, num_workers=4) for x in ['train','val']}\n",
    "x,y = next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db5973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Training Loss: 475.555823, Validation Loss: 9864751.875000\n",
      "[2] Training Loss: 3609.126465, Validation Loss: 1554092.500000\n",
      "[3] Training Loss: 943.928491, Validation Loss: 19215.383301\n",
      "[4] Training Loss: 144.360062, Validation Loss: 22218.823242\n",
      "[5] Training Loss: 157.043590, Validation Loss: 3568.167114\n",
      "[6] Training Loss: 62.932612, Validation Loss: 2506.511688\n",
      "[7] Training Loss: 95.111670, Validation Loss: 12637.155762\n",
      "[8] Training Loss: 84.453060, Validation Loss: 4253.510742\n",
      "[9] Training Loss: 39.183821, Validation Loss: 805.597992\n",
      "[10] Training Loss: 66.666698, Validation Loss: 226.292782\n",
      "[11] Training Loss: 57.670331, Validation Loss: 78.798585\n",
      "[12] Training Loss: 39.600875, Validation Loss: 44.777136\n",
      "[13] Training Loss: 39.447758, Validation Loss: 130.885715\n",
      "[14] Training Loss: 21.647552, Validation Loss: 54.817972\n",
      "[15] Training Loss: 22.418296, Validation Loss: 99.592152\n",
      "[16] Training Loss: 24.822658, Validation Loss: 14.387201\n",
      "[17] Training Loss: 24.030066, Validation Loss: 19.670098\n",
      "[18] Training Loss: 26.646405, Validation Loss: 64.646277\n",
      "[19] Training Loss: 34.957340, Validation Loss: 19.757735\n",
      "[20] Training Loss: 22.935530, Validation Loss: 12.971512\n",
      "[21] Training Loss: 22.790965, Validation Loss: 64.606071\n",
      "[22] Training Loss: 35.659925, Validation Loss: 20.618522\n",
      "[23] Training Loss: 20.927606, Validation Loss: 66.091127\n",
      "[24] Training Loss: 60.703572, Validation Loss: 28.523600\n",
      "[25] Training Loss: 43.827030, Validation Loss: 34.286389\n",
      "[26] Training Loss: 28.245922, Validation Loss: 22.073495\n",
      "[27] Training Loss: 26.173170, Validation Loss: 15.539958\n",
      "[28] Training Loss: 23.020837, Validation Loss: 13.092322\n",
      "[29] Training Loss: 21.974196, Validation Loss: 20.630627\n",
      "[30] Training Loss: 25.587214, Validation Loss: 18.086439\n",
      "[31] Training Loss: 33.275680, Validation Loss: 20.882170\n",
      "[32] Training Loss: 23.751783, Validation Loss: 35.762761\n",
      "[33] Training Loss: 28.028854, Validation Loss: 24.056118\n",
      "[34] Training Loss: 31.209045, Validation Loss: 19.745125\n",
      "[35] Training Loss: 26.542272, Validation Loss: 14.265476\n",
      "[36] Training Loss: 24.620801, Validation Loss: 13.981884\n",
      "[37] Training Loss: 33.597254, Validation Loss: 13.983729\n",
      "[38] Training Loss: 22.576759, Validation Loss: 14.460717\n",
      "[39] Training Loss: 22.942116, Validation Loss: 13.881824\n",
      "[40] Training Loss: 21.942914, Validation Loss: 13.737017\n",
      "[41] Training Loss: 19.953050, Validation Loss: 13.909795\n",
      "[42] Training Loss: 20.709930, Validation Loss: 13.930045\n",
      "[43] Training Loss: 21.223040, Validation Loss: 14.071610\n",
      "[44] Training Loss: 21.397852, Validation Loss: 13.824209\n",
      "[45] Training Loss: 21.213612, Validation Loss: 13.158805\n",
      "[46] Training Loss: 22.345456, Validation Loss: 14.044313\n",
      "[47] Training Loss: 20.049021, Validation Loss: 13.719130\n",
      "[48] Training Loss: 21.514321, Validation Loss: 13.895264\n",
      "[49] Training Loss: 19.720080, Validation Loss: 14.410877\n",
      "[50] Training Loss: 21.471190, Validation Loss: 14.555360\n",
      "[51] Training Loss: 21.795569, Validation Loss: 13.817497\n",
      "[52] Training Loss: 22.089007, Validation Loss: 13.865879\n",
      "[53] Training Loss: 22.080755, Validation Loss: 14.011234\n",
      "[54] Training Loss: 22.642468, Validation Loss: 14.880812\n",
      "[55] Training Loss: 21.242007, Validation Loss: 13.862962\n",
      "[56] Training Loss: 21.191992, Validation Loss: 13.814466\n",
      "[57] Training Loss: 21.727224, Validation Loss: 14.197381\n",
      "[58] Training Loss: 20.340071, Validation Loss: 13.901339\n",
      "[59] Training Loss: 20.728134, Validation Loss: 13.930137\n",
      "[60] Training Loss: 21.105298, Validation Loss: 13.891652\n",
      "[61] Training Loss: 20.807377, Validation Loss: 13.898065\n",
      "[62] Training Loss: 22.117312, Validation Loss: 13.906078\n",
      "[63] Training Loss: 20.738801, Validation Loss: 13.884405\n",
      "[64] Training Loss: 21.902803, Validation Loss: 13.435562\n",
      "[65] Training Loss: 21.138782, Validation Loss: 13.937148\n",
      "[66] Training Loss: 21.121525, Validation Loss: 14.152048\n",
      "[67] Training Loss: 20.811390, Validation Loss: 13.884736\n",
      "[68] Training Loss: 20.824094, Validation Loss: 13.863001\n",
      "[69] Training Loss: 20.535797, Validation Loss: 13.868109\n",
      "[70] Training Loss: 20.227243, Validation Loss: 13.889855\n",
      "[71] Training Loss: 20.924326, Validation Loss: 13.938771\n",
      "[72] Training Loss: 20.691685, Validation Loss: 13.882346\n",
      "[73] Training Loss: 21.363353, Validation Loss: 13.866594\n",
      "[74] Training Loss: 21.595894, Validation Loss: 13.865278\n",
      "[75] Training Loss: 21.698960, Validation Loss: 13.872082\n",
      "[76] Training Loss: 20.801724, Validation Loss: 13.863645\n",
      "[77] Training Loss: 21.014225, Validation Loss: 13.872985\n",
      "[78] Training Loss: 21.768111, Validation Loss: 13.880154\n",
      "[79] Training Loss: 20.628795, Validation Loss: 13.870696\n",
      "[80] Training Loss: 20.865765, Validation Loss: 13.932788\n",
      "[81] Training Loss: 21.361422, Validation Loss: 13.910525\n",
      "[82] Training Loss: 20.562386, Validation Loss: 13.876443\n",
      "[83] Training Loss: 22.256768, Validation Loss: 13.921496\n",
      "[84] Training Loss: 21.319400, Validation Loss: 13.886102\n",
      "[85] Training Loss: 21.232969, Validation Loss: 13.914748\n",
      "[86] Training Loss: 21.098364, Validation Loss: 13.939159\n",
      "[87] Training Loss: 21.339490, Validation Loss: 13.872386\n",
      "[88] Training Loss: 21.409435, Validation Loss: 13.898741\n",
      "[89] Training Loss: 20.500748, Validation Loss: 13.915702\n",
      "[90] Training Loss: 20.957067, Validation Loss: 13.882015\n",
      "[91] Training Loss: 21.560668, Validation Loss: 13.875492\n",
      "[92] Training Loss: 21.304769, Validation Loss: 13.925807\n",
      "[93] Training Loss: 20.969010, Validation Loss: 13.926240\n",
      "[94] Training Loss: 20.961555, Validation Loss: 13.867832\n",
      "[95] Training Loss: 20.505634, Validation Loss: 13.885863\n",
      "[96] Training Loss: 21.475427, Validation Loss: 13.915938\n",
      "[97] Training Loss: 21.005412, Validation Loss: 13.902762\n",
      "[98] Training Loss: 21.556727, Validation Loss: 13.866241\n",
      "[99] Training Loss: 20.942275, Validation Loss: 13.869325\n",
      "[100] Training Loss: 21.332048, Validation Loss: 13.862953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ieisenbraun/anaconda3/lib/python3.8/site-packages/zennit/image.py:184: RuntimeWarning: invalid value encountered in true_divide\n",
      "  array = (array - vmin) / (vmax - vmin)\n",
      "/tmp/ipykernel_804443/588909672.py:18: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  while font.getsize(key)[0] < img_fraction*img.size[0]:\n",
      "/tmp/ipykernel_804443/588909672.py:25: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  w,h = font.getsize(key)\n",
      "/tmp/ipykernel_804443/588909672.py:28: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  true_imgs[key] = img.resize((min_img_width, int(img.height / img.width * min_img_width)), Image.ANTIALIAS)\n",
      "/tmp/ipykernel_804443/588909672.py:34: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  sample = sample.resize((min_img_width,hsize), Image.ANTIALIAS)\n",
      "/tmp/ipykernel_804443/588909672.py:38: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  w,h = font.getsize(\"Original\")\n",
      "/tmp/ipykernel_804443/588909672.py:59: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  while font.getsize(key)[0] < img_fraction*img.size[0]:\n",
      "/tmp/ipykernel_804443/588909672.py:66: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  w,h = font.getsize(key)\n",
      "/tmp/ipykernel_804443/588909672.py:69: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  false_imgs[key] = img.resize((min_img_width, int(img.height / img.width * min_img_width)), Image.ANTIALIAS)\n",
      "/tmp/ipykernel_804443/3967899224.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = float(torch.nn.functional.softmax(pred)[0,1].numpy())\n"
     ]
    }
   ],
   "source": [
    "train_loader = dataloaders['train']\n",
    "test_loader = dataloaders['val']\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_loader, batch_size=32)\n",
    "testloader = torch.utils.data.DataLoader(test_loader, batch_size=32)\n",
    "\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 100\n",
    "\n",
    "with open('result_log.txt', 'w') as f:  \n",
    "    for i in range(0,5):\n",
    "        model=fine_tune_model(train_loader,test_loader,model,optimizer, criterion,epochs)\n",
    "        prunable_channels,pred= evaluate_model(model,i,False)\n",
    "        print(\"iteration {} , pred before pruning:  {}\".format(i, pred))\n",
    "        f.write(\"iteration {} , pred before pruning:  {}\".format(i, pred))\n",
    "        res = {key : dict(sorted(val.items(), key = lambda ele: ele[1],reverse=True)[:2])\n",
    "           for key, val in prunable_channels.items()}\n",
    "        max_vote_ids = get_max_ids(res)\n",
    "        print(\"Voted least relevant concept ids to prune (max vote ids): {}\".format(max_vote_ids))\n",
    "        f.write(\"Voted least relevant concept ids to prune (max vote ids): {}\".format(max_vote_ids))\n",
    "        model = prune_selected_concepts(max_vote_ids,model)\n",
    "        prunable_channels, pred = evaluate_model(model,i,True)\n",
    "        print(\"iteration {} , pred after pruning:  {}\".format(i, pred))\n",
    "        f.write(\"iteration {} , pred after pruning:  {}\".format(i, pred))\n",
    "        model=fine_tune_model(train_loader,test_loader,model,optimizer, criterion,epochs)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caabb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6bd1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9111b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfb92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute heatmap wrt. output 46 (green lizard class)\n",
    "conditions = [{\"y\": 1}]\n",
    "\n",
    "# or use a dictionary for mask_map\n",
    "layer_names = get_layer_names(model, [torch.nn.Conv2d, torch.nn.Linear])\n",
    "mask_map = {name: cc.mask for name in layer_names}\n",
    "attr = attribution(sample, conditions, composite, mask_map=mask_map)\n",
    "zimage.imgify(attr.heatmap, symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c602408",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [{\"model_ft.features.0\": [2],\"y\": 1}]\n",
    "heatmap, _, _, _ = attribution(sample, conditions, composite)\n",
    "zimage.imgify(heatmap, symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [{\"model_ft.features.11\": [55]}]\n",
    "heatmap, _, _, _ = attribution(sample, conditions, composite, start_layer=\"model_ft.features.11\")\n",
    "zimage.imgify(heatmap, symmetric=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7445736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Mitoses\n",
    "path = 'examples_ds_from_MP.train.HTW.train/True'\n",
    "model_pred =0\n",
    "for file in os.listdir(path):\n",
    "    path_to_file = os.path.join(path,file)\n",
    "    #print(path_to_file)\n",
    "    image = Image.open(path_to_file).convert('RGB')\n",
    "    sample = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    img = cv2.imread(path_to_file)\n",
    "    \n",
    "    # Preprocess for the model\n",
    "    # -1 convert from to RGB\n",
    "    original = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # 1. Centercrop\n",
    "    img2 = original[144:144+224,144:144+224]\n",
    "    \n",
    "    # 2. Normalize, so that the mean value for each channel for\n",
    "    # the complete train dataset is 0 and the std 1\n",
    "    img = img2 / 255.\n",
    "    img -= [0.86121925, 0.86814779, 0.88314296] # MEAN-Values\n",
    "    img /= [0.13475281, 0.10909398, 0.09926313] # STD-Values\n",
    "    \n",
    "    # 3. swap axes, convert to tensor and load to GPU\n",
    "    imgtensor = torch.tensor(img.swapaxes(1,2).swapaxes(0,1),dtype=torch.float32)[None]\n",
    "    imgtensor = imgtensor.cuda()\n",
    "    \n",
    "    # 4. Get model prediction\n",
    "    pred = model.predict(imgtensor,logits=True).detach().cpu()\n",
    "    pred = float(torch.nn.functional.softmax(pred)[0,1].numpy())\n",
    "    model_pred+=pred\n",
    "    \n",
    "    #print(torch.exp(model.predict(imgtensor,logits=True).detach().cpu()))\n",
    "model_pred = model_pred / len(os.listdir(path))\n",
    "print(model_pred)\n",
    "    \n",
    "    #plt.title(f'Pred: {pred}')\n",
    "    #plt.imshow(img2)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5709bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FooBarPruningMethod(prune.BasePruningMethod):\n",
    "    \"\"\"Prune every other entry in a tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        mask.view(-1)[::2] = 0\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_img_width = min(i.width for i in true_imgs)\n",
    "total_height = 0\n",
    "for i, img in enumerate(true_imgs):\n",
    "    img.convert('RGB')\n",
    "    if img.width > min_img_width:\n",
    "        true_imgs[i] = img.resize((min_img_width, int(img.height / img.width * min_img_width)), Image.ANTIALIAS)\n",
    "    total_height += true_imgs[i].height\n",
    "img_merge = Image.new(true_imgs[0].mode, (min_img_width, total_height)).convert('RGB')\n",
    "y = 0\n",
    "for img in true_imgs:\n",
    "    img_merge.paste(img, (0, y))\n",
    "\n",
    "    y += img.height\n",
    "img_merge.save('true.jpg')\n",
    "##########################\n",
    "\n",
    "min_img_width = min(i.width for i in false_imgs)\n",
    "total_height = 0\n",
    "for i, img in enumerate(false_imgs):\n",
    "    img.convert('RGB')\n",
    "    if img.width > min_img_width:\n",
    "        false_imgs[i] = img.resize((min_img_width, int(img.height / img.width * min_img_width)), Image.ANTIALIAS)\n",
    "    total_height += false_imgs[i].height\n",
    "img_merge = Image.new(false_imgs[0].mode, (min_img_width, total_height)).convert('RGB')\n",
    "y = 0\n",
    "for img in false_imgs:\n",
    "    img_merge.paste(img, (0, y))\n",
    "\n",
    "    y += img.height\n",
    "img_merge.save('false.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = ChannelConcept()\n",
    "composite = EpsilonPlusFlat([SequentialMergeBatchNorm()])\n",
    "attribution = CondAttribution(model)\n",
    "\n",
    "conditions = [{\"model_ft.features.18\": [30], \"y\": [0]}, {\"model_ft.features.15\": [10], \"y\": [0]}]\n",
    "heatmaps, _, _, _ = attribution(sample, conditions, composite)\n",
    "\n",
    "\n",
    "zimage.imgify(heatmaps, symmetric=True, grid=(1, len(heatmaps)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [{\"model_ft.features.25\": [19]}]\n",
    "\n",
    "heatmap, _, _, _ = attribution(sample, conditions, composite, start_layer=\"model_ft.features.25\")\n",
    "\n",
    "zimage.imgify(heatmap, symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c83c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = get_layer_names(model, [torch.nn.Conv2d, torch.nn.Linear])\n",
    "\n",
    "conditions = [{'y': [0]}]\n",
    "attr = attribution(sample, conditions, composite, record_layer=layer_names)\n",
    "\n",
    "attr.activations['model_ft.features.4'].shape, attr.relevances['model_ft.features.4'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer features.40 has 512 channel concepts\n",
    "rel_c = cc.attribute(attr.relevances['model_ft.features.4'], abs_norm=True)\n",
    "rel_c.shape\n",
    "\n",
    "# the five most relevant concepts\n",
    "concept_ids = torch.argsort(rel_c[0], descending=True)[:6]\n",
    "concept_ids, abs_norm(rel_c[0])[concept_ids]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [{'model_ft.features.4': [id], 'y': [0]} for id in concept_ids]\n",
    "\n",
    "heatmap, _, _, _ = attribution(sample, conditions, composite)\n",
    "\n",
    "zimage.imgify(heatmap, symmetric=True, grid=(1, len(concept_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36845d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [{'model_ft.features.4': [id], 'y': [0]} for id in np.arange(0, 128)]\n",
    "\n",
    "for attr in attribution.generate(sample, conditions, composite, record_layer=layer_names, batch_size=1):\n",
    "    pass\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros(512, 512)\n",
    "mask[:, 180:] = 1\n",
    "\n",
    "zimage.imgify(mask, symmetric=True)\n",
    "rel_c = []\n",
    "for attr in attribution.generate(sample, conditions, composite, record_layer=layer_names, batch_size=1):\n",
    "    \n",
    "    masked = attr.heatmap * mask[None, :, :]\n",
    "    rel_c.append(torch.sum(masked, dim=(1, 2)))\n",
    "\n",
    "rel_c = torch.cat(rel_c)\n",
    "\n",
    "indices = torch.argsort(rel_c, descending=True)[:5]\n",
    "# we norm here, so that we clearly see the contribution inside the masked region as percentage\n",
    "indices, abs_norm(rel_c)[indices]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [{\"y\": [0], 'model_ft.features.4': [9]}]\n",
    "\n",
    "attr = attribution(sample, conditions, composite, record_layer=[\"model_ft.features.2\"])\n",
    "\n",
    "rel_c = cc.attribute(attr.relevances[\"model_ft.features.2\"], abs_norm=True)\n",
    "\n",
    "# five concepts in features.37 that contributed the most to the activation of channel 469 in features.40\n",
    "# while being relevant for the classification of the lizard class\n",
    "torch.argsort(rel_c, descending=True)[0, :5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = trace_model_graph(model, sample, layer_names)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61900cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.find_input_layers('model_ft.features.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff123fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = get_layer_names(model, [torch.nn.Conv2d, torch.nn.Linear])\n",
    "\n",
    "layer_map = {name: cc for name in layer_names}\n",
    "attgraph = AttributionGraph(attribution, graph, layer_map)\n",
    "\n",
    "# decompose concept 71 in features.40 w.r.t. target 46 (lizard class)\n",
    "# width=[5, 2] returns first the 5 most relevant concepts in the first lower-level layer\n",
    "# and in the second iteration returns for each of the 5 most relevant concepts again the two\n",
    "# most relevant concepts in the following lower-level layer\n",
    "nodes, connections = attgraph(sample, composite, 82, 'model_ft.features.4', 0, width=[5, 2], abs_norm=True)\n",
    "print(\"Nodes:\\n\", nodes, \"\\nConnections:\\n\", connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections[('model_ft.features.4', 82)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Mitoses\n",
    "path = 'examples_ds_from_MP.train.HTW.train/True'\n",
    "for file in os.listdir(path):\n",
    "    path_to_file = os.path.join(path,file)\n",
    "    print(path_to_file)\n",
    "    img = cv2.imread(path_to_file)\n",
    "    \n",
    "    # Preprocess for the model\n",
    "    # -1 convert from to RGB\n",
    "    original = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # 1. Centercrop\n",
    "    img2 = original[144:144+224,144:144+224]\n",
    "    \n",
    "    # 2. Normalize, so that the mean value for each channel for\n",
    "    # the complete train dataset is 0 and the std 1\n",
    "    img = img2 / 255.\n",
    "    img -= [0.86121925, 0.86814779, 0.88314296] # MEAN-Values\n",
    "    img /= [0.13475281, 0.10909398, 0.09926313] # STD-Values\n",
    "    \n",
    "    # 3. swap axes, convert to tensor and load to GPU\n",
    "    imgtensor = torch.tensor(img.swapaxes(1,2).swapaxes(0,1),dtype=torch.float32)[None]\n",
    "    \n",
    "    # 4. Get model prediction\n",
    "    pred = model.predict(imgtensor,logits=True).detach()\n",
    "    pred = float(torch.nn.functional.softmax(pred)[0,1].numpy())\n",
    "    \n",
    "    print(torch.exp(model.predict(imgtensor,logits=True).detach()))\n",
    "    print(pred)\n",
    "    \n",
    "    plt.title(f'Pred: {pred}')\n",
    "    plt.imshow(img2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Non-Mitoses\n",
    "path = 'examples_ds_from_MP.train.HTW.train/False'\n",
    "for file in os.listdir(path):\n",
    "    path_to_file = os.path.join(path,file)\n",
    "    print(path_to_file)\n",
    "    img = cv2.imread(path_to_file)\n",
    "    \n",
    "    # Preprocess for the model\n",
    "    # -1 convert from to RGB\n",
    "    original = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # 1. Centercrop\n",
    "    img2 = original[144:144+224,144:144+224]\n",
    "    \n",
    "    # 2. Normalize, so that the mean value for each channel for\n",
    "    # the complete train dataset is 0 and the std 1\n",
    "    img = img2 / 255.\n",
    "    img -= [0.86121925, 0.86814779, 0.88314296] # MEAN-Values\n",
    "    img /= [0.13475281, 0.10909398, 0.09926313] # STD-Values\n",
    "    \n",
    "    # 3. swap axes, convert to tensor and load to GPU\n",
    "    imgtensor = torch.tensor(img.swapaxes(1,2).swapaxes(0,1),dtype=torch.float32)[None]\n",
    "    \n",
    "    # 4. Get model prediction\n",
    "    pred = model.predict(imgtensor,logits=True).detach()\n",
    "    pred = float(torch.nn.functional.softmax(pred)[0,1].numpy())\n",
    "    \n",
    "    print(torch.exp(model.predict(imgtensor,logits=True).detach())\n",
    "    print(pred)\n",
    "    \n",
    "    plt.title(f'Pred: {pred}')\n",
    "    plt.imshow(img2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf304fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a9754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa7678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56806651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd31be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0ec85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
